# policy configurations for robot

[rl]
gamma = 0.9


[om]
cell_num = 4
cell_size = 1
om_channel_size = 3


[action_space]
kinematics = holonomic
# action space size is speed_samples * rotation_samples + 1
speed_samples = 5
rotation_samples = 32
sampling = exponential
query_env = true


[cadrl]
mlp_dims = 150, 100, 100, 1
multiagent_training = false


[lstm_rl]
global_state_dim = 50
mlp1_dims = 150, 100, 100, 50
mlp2_dims = 150, 100, 100, 1
multiagent_training = true
with_om = false
with_interaction_module = false


[srl]
mlp1_dims = 150, 100, 100, 50
mlp2_dims = 150, 100, 100, 1
multiagent_training = true
with_om = false


[sarl]
mlp1_dims = 150, 100
mlp2_dims = 100, 50
attention_dims = 100, 100, 1
mlp3_dims = 150, 100, 100, 1
multiagent_training = true
with_om = false
with_global_state = true

[frozen]
with_pretrained = true
frozen_training = false
model_file = sgan_models/checkpoint_obs4.pt
obs_len = 4
pred_len = 8
mlp_dim = 1024
num_layers = 1
pooling_type = pool_net
pool_every_timestep = 1
dropout = 0
bottleneck_dim = 1024
neighborhood_size = 2.0
grid_size = 8
batch_norm = 0
multiagent_training = true
policy_learning = true
